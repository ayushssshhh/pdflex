// endpoint for asking question to pdf

import { db } from "@/db";
import { sendMessageValidator } from "@/lib/validator/SendMessageValidator";
import { getKindeServerSession } from "@kinde-oss/kinde-auth-nextjs/server";
import { NextRequest } from "next/server";
import { pinecone } from "@/lib/validator/pinecone";
import { OpenAIEmbeddings } from "langchain/embeddings/openai"
import { PineconeStore } from 'langchain/vectorstores/pinecone'
import { openai } from "@/lib/openai";
import {OpenAIStream , StreamingTextResponse} from "ai"

export const POST = (async (req: NextRequest) => {
    const body = await req.json()

    // making sure user if auth
    const { getUser } = getKindeServerSession()
    const user = await getUser()
    const userId = user?.id

    // if user not logged in
    if (!userId) {
        return new Response('Unauthorize', { status: 401 })
    }

    // geting body content -> file and message
    const { fileId, message } = sendMessageValidator.parse(body)

    // fething file for which message req is made
    const file = await db.file.findFirst({
        where: {
            id: fileId,
            userId,
        }
    })

    // if file not found
    if (!file) {
        return new Response('Not Found', { status: 404 })
    }

    // storing message in messageDb
    await db.message.create({
        data: {
            text: message,
            isUserMessage: true,
            userId,
            fileId
        }
    })

    // creating answer from pdf:
    // 1. index entire pdf into vector
    // 2. index message from user into vector
    // 3. match the both index and find most relevant match

    const pineconeIndex = pinecone.Index("pdflex")

    // use to generate vector from text
    const embeddings = new OpenAIEmbeddings({
        openAIApiKey: process.env.OPENAI_API_KEY
    })

    // search most relevant vectorPage with messageVector 
    const vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
        pineconeIndex,
        namespace: file.id
    })

    // extracting 4 closest vector
    const results = await vectorStore.similaritySearch(message, 4)

    // taking account of previous 6 messages for better accuracy
    const prevMessages = await db.message.findMany({
        where: {
            fileId,
        },
        orderBy: {
            createdAt: "asc"
        },
        take: 6
    })

    // formatting prev message for openAi i/p
    const formattedPrevMessages = prevMessages.map((msg) => ({
        role: msg.isUserMessage ? "user" as const : "assistant" as const,
        content: msg.text
    }))

    // sending req to openAi

    const response = await openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        temperature: 0,
        stream: true,  //streaming response in realtime
        messages: [
            {
                role: 'system',
                content: 'Use the following pieces of context (or previous conversaton if needed) to answer the users question in markdown format.',
            },
            {
                role: 'user',
                content:
                `
                    Use the following pieces of context (or previous conversaton if needed) to answer the users question in markdown format. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.
              
                    \n----------------\n
        
                    PREVIOUS CONVERSATION:
                    ${formattedPrevMessages.map((message) => {
                        if (message.role === 'user') return `User: ${message.content}\n`
                        return `Assistant: ${message.content}\n`
                    })}
        
                    \n----------------\n
        
                    CONTEXT:
                    ${results.map((r) => r.pageContent).join('\n\n')}
        
                    USER INPUT: ${message}
                `,
            },
        ],
    })

    
    // getting text from response generated by openAI
    const stream = OpenAIStream(response , {
        async onCompletion(completion) { //completion -> one long setence send by open ai
    
            // saving ans in messageDb
            await db.message.create({
                data: {
                    text: completion,
                    isUserMessage: false,
                    fileId,
                    userId
                }
            })
        }
    })

    // streaming response on realtime using custom route

    // console.log("user : " + message);
    // console.log("api : " + stream);

    return new StreamingTextResponse(stream)
})